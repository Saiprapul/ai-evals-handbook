<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG Evaluation Framework | AI Evals Handbook</title>
    <meta name="description"
        content="A comprehensive guide to evaluating RAG systems. Learn about the 'RAG Triad', specific metrics like Context Precision and Faithfulness, and how to debug retrieval failures in production.">
    <link rel="canonical" href="https://saiprapul.github.io/ai-evals-handbook/frameworks/rag-evaluation.html">
    <meta property="og:title" content="RAG Evaluation Framework | AI Evals Handbook">
    <meta property="og:description" content="When retrieval fails, generation hallucinates. Evaluate both or evaluate neither.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://saiprapul.github.io/ai-evals-handbook/frameworks/rag-evaluation.html">
    <meta property="og:site_name" content="AI Evals Handbook">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="RAG Evaluation Framework | AI Evals Handbook">
    <meta name="twitter:description" content="When retrieval fails, generation hallucinates. Evaluate both or evaluate neither.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="../css/framework-page.css">
    <link rel="stylesheet" href="../css/docs.css">
</head>

<body>
    <header class="docs-header">
        <div class="container">
            <a href="../" class="docs-logo"><span class="logo-icon">E</span>AI Evals Handbook</a>
            <div class="header-actions">
                <a href="../why-evals-matter.html" class="btn btn-sm btn-primary">Start Here</a>
                <a href="https://github.com/saiprapul/ai-evals-handbook" target="_blank" class="btn btn-sm">GitHub</a>
            </div>
        </div>
    </header>

    <div class="docs-layout">
        <aside class="docs-sidebar" id="sidebar"></aside>

        <main class="docs-main">
            <div class="docs-content">
                <div class="breadcrumbs">
                    <a href="../">Home</a>
                    <span class="separator">/</span>
                    <span>Eval Design</span>
                    <span class="separator">/</span>
                    <span>RAG Evals</span>
                </div>

                <h1>RAG Evaluation Strategies</h1>
                <p class="framework-subtitle">Deconstruct your pipeline into Retrieval and Generation validation.</p>

                <hr style="margin: 2rem 0; border: none; border-bottom: 1px solid var(--border);">

                <div class="content">


                    <section>


                        <h2 id="quick-answer">Quick Answer</h2>


                        <p>RAG systems must be evaluated on retrieval and generation. The RAG triad - Query, Context, Response - prevents confusing retrieval errors with model errors.</p>


                    </section>


                    <section>


                        <h2 id="tldr">TL;DR</h2>


                        <ul>


                            <li>Measure context precision and recall.</li>


                            <li>Evaluate faithfulness and answer correctness.</li>


                            <li>Debug failures by isolating retrieval vs. generation.</li>


                        </ul>


                    </section>


                    <section>


                        <h2 id="faq">FAQ</h2>


                        <h3>What is the RAG triad?</h3>


                        <p>It separates evaluation into the user query, retrieved context, and final response so you can find the failing layer.</p>


                        <h3>Which metrics matter most?</h3>


                        <p>Context precision/recall for retrieval and faithfulness for generation are usually the highest signal.</p>


                        <h3>How do I debug a poor answer?</h3>


                        <p>Check whether the correct document was retrieved. If not, fix retrieval; if yes, fix prompting or reasoning.</p>


                    </section>
                    <section id="introduction">
                        <h2>The "Why" of RAG Evals</h2>
                        <p>Evaluate a RAG system as a single black box, and you'll never know <em>why</em> it failed.
                            Did it miss the document? Or did it have the document but hallucinates anyway?</p>
                        <p>We break RAG evaluation into the <strong>RAG Triad</strong>: Query, Context, and Response.
                        </p>

                        <div class="diagram-card">
                            <svg class="diagram-svg" viewBox="0 0 900 220" role="img"
                                aria-label="RAG flow from user query to retriever to LLM and answer">
                                <defs>
                                    <marker id="rag-arrow" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="6"
                                        markerHeight="6" orient="auto-start-reverse">
                                        <path d="M 0 0 L 10 5 L 0 10 z" class="diagram-arrow"></path>
                                    </marker>
                                </defs>
                                <rect class="diagram-node" x="40" y="70" width="160" height="60" rx="12"></rect>
                                <text x="120" y="103" text-anchor="middle" class="diagram-label">User Query</text>

                                <rect class="diagram-node" x="250" y="70" width="160" height="60" rx="12"></rect>
                                <text x="330" y="103" text-anchor="middle" class="diagram-label">Retriever</text>

                                <rect class="diagram-node diagram-node-accent" x="460" y="70" width="160" height="60"
                                    rx="12"></rect>
                                <text x="540" y="95" text-anchor="middle" class="diagram-label">LLM</text>
                                <text x="540" y="115" text-anchor="middle" class="diagram-note">Reason + Answer</text>

                                <rect class="diagram-node" x="670" y="70" width="180" height="60" rx="12"></rect>
                                <text x="760" y="103" text-anchor="middle" class="diagram-label">Final Answer</text>

                                <path class="diagram-line-solid" d="M 200 100 L 250 100" marker-end="url(#rag-arrow)">
                                </path>
                                <path class="diagram-line-solid" d="M 410 100 L 460 100" marker-end="url(#rag-arrow)">
                                </path>
                                <path class="diagram-line-solid" d="M 620 100 L 670 100" marker-end="url(#rag-arrow)">
                                </path>

                                <path class="diagram-line-accent" d="M 120 70 L 120 40 L 540 40 L 540 70"
                                    marker-end="url(#rag-arrow)"></path>
                                <text x="330" y="32" text-anchor="middle" class="diagram-note">User question</text>
                                <text x="330" y="122" text-anchor="middle" class="diagram-note">Context</text>
                            </svg>
                        </div>
                    </section>

                    <section id="retrieval">
                        <h2>1. Retrieval Metrics (The Search Engine)</h2>
                        <p>Before an LLM even sees your prompt, your retrieval system needs to find the right data. If
                            garbage goes in, garbage comes out.</p>

                        <div class="card-grid">
                            <div class="metric-card">
                                <h3>Context Precision</h3>
                                <p><strong>Definition:</strong> What proportion of retrieved chunks are actually
                                    relevant?</p>
                                <p><strong>Why it matters:</strong> Low precision floods the LLM context window with
                                    noise, increasing cost and "lost in the middle" errors.</p>
                                <div class="formula"><code>relevant_chunks / k_retrieved_chunks</code></div>
                            </div>

                            <div class="metric-card">
                                <h3>Context Recall</h3>
                                <p><strong>Definition:</strong> Did we retrieve <em>all</em> the necessary information
                                    to answer the question?</p>
                                <p><strong>Why it matters:</strong> If the legal statute is in Chunk #50 but you only
                                    retrieved top-5, the model <em>cannot</em> be correct without hallucinating.</p>
                            </div>
                        </div>

                        <div class="callout warning">
                            <h4>The "Lost in the Middle" Phenomenon</h4>
                            <p>Research shows LLMs prioritize information at the START and END of their context window.
                                High precision is often more valuable than high recall if it means shorter contexts.</p>
                        </div>
                    </section>

                    <section id="generation">
                        <h2>2. Generation Metrics (The Writer)</h2>
                        <p>Once you have the right context, did the model use it correctly?</p>

                        <h3>Faithfulness</h3>
                        <p>This checks for <strong>Hallucinations</strong>. A faithful answer contains only information
                            found in the retrieved context.</p>
                        <ul>
                            <li><strong>Evaluation Method:</strong> Use an "LLM-as-Judge" to extract claims from the
                                answer and verify each claim against the source chunks.</li>
                            <li><strong>Score:</strong> 0.0 (Pure Hallucination) to 1.0 (Fully Grounded).</li>
                        </ul>

                        <h3>Answer Relevancy</h3>
                        <p>A model can be faithful ("The sky is blue") but irrelevant to the question ("What is the
                            capital of France?"). Relevancy measures the semantic similarity between the Query and the
                            Response.</p>
                    </section>

                    <section id="implementation">
                        <h2>Implementation Strategy</h2>
                        <p>Do not rely on vibes. Build a <code>GoldenDataset</code> of (Question, Context_Ids,
                            Ground_Truth) tuples.</p>

                        <div class="code-block">
                            <div class="code-header"><span>rag_eval_example.py</span></div>
                            <pre><code><span class="keyword">def</span> <span class="function">evaluate_retrieval</span>(retrieved_ids, expected_ids):
    <span class="comment"># Calculate Intersection over Union (IoU) or simple Recall</span>
    intersection = <span class="builtin">len</span>(<span class="builtin">set</span>(retrieved_ids) & <span class="builtin">set</span>(expected_ids))
    recall = intersection / <span class="builtin">len</span>(expected_ids)
    <span class="keyword">return</span> recall

<span class="keyword">def</span> <span class="function">evaluate_generation_faithfulness</span>(answer, context_text):
    <span class="comment"># Use a smaller, cheaper model (e.g. GPT-3.5) to verify claims</span>
    prompt = f<span class="string">"""
    Context: {context_text}
    Answer: {answer}
    
    Identify any claims in the Answer not supported by the Context.
    Return Score (0-1).
    """</span>
    <span class="keyword">return</span> llm_judge(prompt)</code></pre>
                        </div>
                        <p><a href="../code/rag_evaluator.html" class="btn btn-sm btn-primary"
                                style="margin-top: 16px;">See Full Evaluator Code â†’</a></p>
                    </section>
                </div>

                <div class="docs-footer-nav">
                    <a href="governance.html" class="nav-card">
                        <span class="label">Previous</span>
                        <span class="title">&larr; Risk-First Scoping</span>
                    </a>
                    <a href="llm-as-judge.html" class="nav-card">
                        <span class="label">Next</span>
                        <span class="title">LLM-as-Judge &rarr;</span>
                    </a>
                </div>
            </div>
        </main>
    </div>

    <script src="../js/main.js"></script>
    <script src="../js/sidebar.js"></script>
</body>

</html>
