<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-as-Judge Patterns | AI Evals Handbook</title>
    <meta name="description"
        content="A deep dive into using LLMs to evaluate other LLMs. Learn about G-Eval, Prometheus, handling positional bias, and reducing evaluation costs.">
    <link rel="canonical" href="https://saiprapul.github.io/ai-evals-handbook/frameworks/llm-as-judge.html">
    <meta property="og:title" content="LLM-as-Judge Patterns | AI Evals Handbook">
    <meta property="og:description" content="Using strong models to evaluate the outputs of weaker models. Learn about G-Eval, biases, and cost reduction.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://saiprapul.github.io/ai-evals-handbook/frameworks/llm-as-judge.html">
    <meta property="og:site_name" content="AI Evals Handbook">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="LLM-as-Judge Patterns | AI Evals Handbook">
    <meta name="twitter:description" content="Using strong models to evaluate the outputs of weaker models.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="../css/framework-page.css">
    <link rel="stylesheet" href="../css/docs.css">
</head>

<body>
    <header class="docs-header">
        <div class="container">
            <a href="../" class="docs-logo"><span class="logo-icon">E</span>AI Evals Handbook</a>
            <div class="header-actions">
                <a href="../why-evals-matter.html" class="btn btn-sm btn-primary">Start Here</a>
                <a href="https://github.com/saiprapul/ai-evals-handbook" target="_blank" class="btn btn-sm">GitHub</a>
            </div>
        </div>
    </header>
    <div class="docs-layout">
        <aside class="docs-sidebar" id="sidebar"></aside>
        <main class="docs-main">
            <div class="docs-content">
                <div class="breadcrumbs">
                    <a href="../">Home</a>
                    <span class="separator">/</span>
                    <span>Eval Design</span>
                    <span class="separator">/</span>
                    <span>LLM-as-Judge</span>
                </div>
                <h1>LLM-as-Judge</h1>
                <p class="framework-subtitle">Using strong models (like GPT-4) to evaluate the outputs of weaker models
                    (like Llama-2 or older GPT versions).</p>
                <hr style="margin: 2rem 0; border: none; border-bottom: 1px solid var(--border);">

                <div class="content">


                    <section>


                        <h2 id="quick-answer">Quick Answer</h2>


                        <p>LLM-as-judge uses models to score other models with a rubric. It can scale evaluation but must be calibrated and audited for bias.</p>


                    </section>


                    <section>


                        <h2 id="tldr">TL;DR</h2>


                        <ul>


                            <li>Use clear rubrics and few-shot examples.</li>


                            <li>Validate with human review and spot checks.</li>


                            <li>Monitor judge drift and disagreement rates.</li>


                        </ul>


                    </section>


                    <section>


                        <h2 id="faq">FAQ</h2>


                        <h3>When is LLM-as-judge reliable?</h3>


                        <p>It works best for structured rubrics and factuality checks; it is weaker for subjective or ambiguous tasks.</p>


                        <h3>How do I calibrate the judge?</h3>


                        <p>Compare judge scores against human labels on a validation set and adjust prompts or thresholds.</p>


                        <h3>What are common failure modes?</h3>


                        <p>Overconfidence, preference for verbosity, and bias toward certain styles or providers.</p>


                    </section>
                    <section id="the-concept">
                        <h2>The Core Concept</h2>
                        <p>In many tasks (creative writing, reasoning, summarization), there is no single "correct"
                            answer string. Exact match metrics (BLEU, ROUGE) correlate poorly with human judgment.</p>
                        <p><strong>LLM-as-Judge</strong> uses a high-capacity model to act as a human proxy, reading the
                            input, the output, and a scoring rubric to assign a grade.</p>

                        <div class="diagram-card">
                            <svg class="diagram-svg" viewBox="0 0 900 240" role="img"
                                aria-label="LLM-as-judge flow from user input to model output to judge score">
                                <defs>
                                    <marker id="judge-arrow" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="6"
                                        markerHeight="6" orient="auto-start-reverse">
                                        <path d="M 0 0 L 10 5 L 0 10 z" class="diagram-arrow"></path>
                                    </marker>
                                </defs>

                                <rect class="diagram-node" x="40" y="80" width="160" height="60" rx="12"></rect>
                                <text x="120" y="108" text-anchor="middle" class="diagram-label">User Input</text>
                                <text x="120" y="128" text-anchor="middle" class="diagram-note">Article + Prompt</text>

                                <rect class="diagram-node" x="240" y="80" width="170" height="60" rx="12"></rect>
                                <text x="325" y="108" text-anchor="middle" class="diagram-label">Model A (Weak)</text>
                                <text x="325" y="128" text-anchor="middle" class="diagram-note">Draft Summary</text>

                                <rect class="diagram-node diagram-node-accent" x="460" y="60" width="170" height="90"
                                    rx="12"></rect>
                                <text x="545" y="92" text-anchor="middle" class="diagram-label">Judge (LLM)</text>
                                <text x="545" y="112" text-anchor="middle" class="diagram-note">Rubric + Checks</text>
                                <text x="545" y="132" text-anchor="middle" class="diagram-note">Hallucination, Style</text>

                                <rect class="diagram-node" x="670" y="80" width="190" height="60" rx="12"></rect>
                                <text x="765" y="108" text-anchor="middle" class="diagram-label">Score + Rationale</text>

                                <path class="diagram-line-solid" d="M 200 110 L 240 110"
                                    marker-end="url(#judge-arrow)"></path>
                                <path class="diagram-line-solid" d="M 410 110 L 460 110"
                                    marker-end="url(#judge-arrow)"></path>
                                <path class="diagram-line-solid" d="M 630 110 L 670 110"
                                    marker-end="url(#judge-arrow)"></path>

                                <path class="diagram-line" d="M 120 80 L 120 40 L 545 40 L 545 60"
                                    marker-end="url(#judge-arrow)"></path>
                                <text x="330" y="34" text-anchor="middle" class="diagram-note">Original + Rubric</text>
                            </svg>
                        </div>
                    </section>

                    <section id="approaches">
                        <h2>Evaluation Approaches</h2>

                        <h3>1. Reference-Free Evaluation</h3>
                        <p>The judge sees only the <code>Input</code> and the <code>Output</code>. It decides if the
                            output is "good" based on internal knowledge and the prompt instructions.</p>
                        <ul>
                            <li><strong>Use Case:</strong> Creative writing, Coding assistants (does it compile?),
                                General chat.</li>
                            <li><strong>Pros:</strong> Doesn't require a "Golden Dataset" of answers.</li>
                            <li><strong>Cons:</strong> Subjective.</li>
                        </ul>

                        <h3>2. Reference-Based Evaluation</h3>
                        <p>The judge compares the <code>Output</code> against a <code>Gold Reference</code>.</p>
                        <ul>
                            <li><strong>Use Case:</strong> RAG, Fact-extraction.</li>
                            <li><strong>Pros:</strong> More grounded.</li>
                            <li><strong>Cons:</strong> Expensive to create references.</li>
                        </ul>
                    </section>

                    <section id="biases">
                        <h2>Biases & Limitations</h2>
                        <p>Judges are not perfect. Research (Zheng et al., 2023) has identified distinct biases:</p>

                        <div class="comparison-table">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Bias Type</th>
                                        <th>Description</th>
                                        <th>Mitigation Strategy</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Position Bias</strong></td>
                                        <td>In pairwise comparison, models prefer the first option presented.</td>
                                        <td>Run eval twice, swapping order (A vs B, then B vs A).</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Verbosity Bias</strong></td>
                                        <td>Models prefer longer answers, even if they are repetitive or fluff.</td>
                                        <td>Explicitly penalize length in the system prompt; Use "conciseness" metrics.
                                        </td>
                                    </tr>
                                    <tr>
                                        <td><strong>Self-Preference</strong></td>
                                        <td>GPT-4 tends to rate GPT-4 outputs higher than Claude/Llama outputs.</td>
                                        <td>Use a neutral judge or ensemble of judges.</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </section>

                    <section id="prompting">
                        <h2>The "G-Eval" Prompt Pattern</h2>
                        <p>The most effective way to use LLM-as-Judge is the <strong>Chain-of-Thought Rubric</strong>
                            pattern (popularized by the G-Eval paper).</p>
                        <div class="code-block">
                            <div class="code-header"><span>Judge System Prompt</span></div>
                            <pre><code>You are an expert evaluator for search relevance.
    
<strong>Task:</strong> Rate the relevance of the AI Answer to the User Query on a scale of 1-5.

<strong>Rubric:</strong>
1 - Completely irrelevant.
2 - Tangential connection, misses intent.
3 - Addresses the topic but lacks detail or specific accuracy.
4 - Good answer, minor omissions.
5 - Perfect output, helpful, concise, and accurate.

<strong>Steps:</strong>
1. Read the Query and identify the user intent.
2. Read the Answer.
3. Compare the Answer claims to the intent.
4. Assign a score based ONLY on the rubric.

Output format: JSON { "score": int, "reasoning": "string" }</code></pre>
                        </div>
                    </section>
                </div>
                <div class="docs-footer-nav">
                    <a href="rag-evaluation.html" class="nav-card">
                        <span class="label">Previous</span>
                        <span class="title">&larr; RAG Evals</span>
                    </a>
                    <a href="human-in-the-loop.html" class="nav-card">
                        <span class="label">Next</span>
                        <span class="title">Human-in-the-Loop &rarr;</span>
                    </a>
                </div>
            </div>
        </main>
    </div>
    <script src="../js/main.js"></script>
    <script src="../js/sidebar.js"></script>
</body>

</html>
