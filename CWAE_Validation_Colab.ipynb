{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CWAE: Consequence-Weighted AI Evaluation\n",
        "## Validation on BIRD Benchmark\n",
        "\n",
        "This notebook validates the CWAE framework against the real BIRD Text-to-SQL benchmark.\n",
        "\n",
        "**What we'll do:**\n",
        "1. Install CWAE and dependencies\n",
        "2. Download BIRD benchmark data\n",
        "3. Load published model predictions (or generate with a simple baseline)\n",
        "4. Run both standard EX and CWAE evaluation\n",
        "5. Compare rankings and demonstrate the materiality effect\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q numpy pandas tqdm requests\n",
        "\n",
        "# Clone the CWAE repository (or upload the files)\n",
        "!git clone https://github.com/YOUR_USERNAME/cwae.git 2>/dev/null || echo \"Using local files\"\n",
        "\n",
        "# If you uploaded the cwae folder directly, uncomment this:\n",
        "# (Upload cwae folder to Colab files panel first)\n",
        "import sys\n",
        "sys.path.insert(0, '/content/cwae')\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Paste the core CWAE code directly (no git needed)\n",
        "# This cell contains the minimal CWAE implementation\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/cwae_lib', exist_ok=True)\n",
        "\n",
        "# Write types.py\n",
        "types_code = '''\n",
        "from enum import Enum\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, List, Dict, Any\n",
        "\n",
        "class MaterialityTier(Enum):\n",
        "    CRITICAL = \"critical\"\n",
        "    OPERATIONAL = \"operational\"\n",
        "    INFORMATIONAL = \"informational\"\n",
        "\n",
        "@dataclass\n",
        "class MaterialityConfig:\n",
        "    tier: MaterialityTier\n",
        "    tolerance_threshold: float\n",
        "    penalty_multiplier: float\n",
        "    confidence_discount: float\n",
        "    \n",
        "    @classmethod\n",
        "    def default_critical(cls):\n",
        "        return cls(MaterialityTier.CRITICAL, 0.005, 2.0, 0.7)\n",
        "    \n",
        "    @classmethod\n",
        "    def default_operational(cls):\n",
        "        return cls(MaterialityTier.OPERATIONAL, 0.05, 1.0, 0.85)\n",
        "    \n",
        "    @classmethod\n",
        "    def default_informational(cls):\n",
        "        return cls(MaterialityTier.INFORMATIONAL, 0.10, 0.5, 1.0)\n",
        "\n",
        "@dataclass\n",
        "class FMSResult:\n",
        "    score: float\n",
        "    quantitative_loss: float\n",
        "    qualitative_loss: float\n",
        "    materiality_loss: float\n",
        "    tier: MaterialityTier\n",
        "    relative_error: float\n",
        "    within_tolerance: bool\n",
        "    predicted_value: Any\n",
        "    gold_value: Any\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "'''\n",
        "\n",
        "with open('/content/cwae_lib/types.py', 'w') as f:\n",
        "    f.write(types_code)\n",
        "\n",
        "# Write fms.py\n",
        "fms_code = '''\n",
        "from typing import Any, Optional, Union\n",
        "from .types import MaterialityTier, MaterialityConfig, FMSResult\n",
        "\n",
        "class FMS:\n",
        "    def __init__(self, tier=MaterialityTier.OPERATIONAL, config=None, w_quant=0.7, w_qual=0.3, epsilon=1e-10):\n",
        "        self.tier = tier\n",
        "        self.config = config or self._default_config(tier)\n",
        "        self.w_quant = w_quant\n",
        "        self.w_qual = w_qual\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def _default_config(self, tier):\n",
        "        configs = {\n",
        "            MaterialityTier.CRITICAL: MaterialityConfig.default_critical(),\n",
        "            MaterialityTier.OPERATIONAL: MaterialityConfig.default_operational(),\n",
        "            MaterialityTier.INFORMATIONAL: MaterialityConfig.default_informational(),\n",
        "        }\n",
        "        return configs[tier]\n",
        "    \n",
        "    def compute_relative_error(self, predicted, gold):\n",
        "        return abs(predicted - gold) / max(abs(gold), self.epsilon)\n",
        "    \n",
        "    def compute_quantitative_loss(self, relative_error):\n",
        "        tau = self.config.tolerance_threshold\n",
        "        alpha = self.config.penalty_multiplier\n",
        "        if relative_error <= tau:\n",
        "            return 0.0\n",
        "        scaled = alpha * (relative_error - tau) / (1 - tau)\n",
        "        return min(1.0, scaled)\n",
        "    \n",
        "    def score(self, predicted_value, gold_value, predicted_sql=None, gold_sql=None):\n",
        "        relative_error = self.compute_relative_error(predicted_value, gold_value)\n",
        "        within_tolerance = relative_error <= self.config.tolerance_threshold\n",
        "        l_quant = self.compute_quantitative_loss(relative_error)\n",
        "        l_qual = 0.0  # Simplified - no SQL comparison\n",
        "        l_mat = self.w_quant * l_quant + self.w_qual * l_qual\n",
        "        fms_score = 1.0 - l_mat\n",
        "        \n",
        "        return FMSResult(\n",
        "            score=fms_score, quantitative_loss=l_quant, qualitative_loss=l_qual,\n",
        "            materiality_loss=l_mat, tier=self.tier, relative_error=relative_error,\n",
        "            within_tolerance=within_tolerance, predicted_value=predicted_value,\n",
        "            gold_value=gold_value\n",
        "        )\n",
        "    \n",
        "    def score_binary(self, is_correct, predicted_sql=None, gold_sql=None):\n",
        "        if is_correct:\n",
        "            return FMSResult(\n",
        "                score=1.0, quantitative_loss=0.0, qualitative_loss=0.0,\n",
        "                materiality_loss=0.0, tier=self.tier, relative_error=0.0,\n",
        "                within_tolerance=True, predicted_value=1, gold_value=1\n",
        "            )\n",
        "        else:\n",
        "            l_quant = min(1.0, self.config.penalty_multiplier)\n",
        "            l_mat = self.w_quant * l_quant\n",
        "            return FMSResult(\n",
        "                score=max(0.0, 1.0 - l_mat), quantitative_loss=l_quant, qualitative_loss=0.0,\n",
        "                materiality_loss=l_mat, tier=self.tier, relative_error=1.0,\n",
        "                within_tolerance=False, predicted_value=0, gold_value=1\n",
        "            )\n",
        "'''\n",
        "\n",
        "with open('/content/cwae_lib/fms.py', 'w') as f:\n",
        "    f.write(fms_code)\n",
        "\n",
        "# Write classifier.py\n",
        "classifier_code = '''\n",
        "import re\n",
        "from .types import MaterialityTier\n",
        "\n",
        "class MaterialityClassifier:\n",
        "    CRITICAL_PATTERNS = [\n",
        "        r\"\\\\b(revenue|profit|loss|income|expense|ebitda|margin)\\\\b\",\n",
        "        r\"\\\\b(total|sum|aggregate)\\\\b.*\\\\b(revenue|sales|cost|amount)\\\\b\",\n",
        "        r\"\\\\b(net|gross)\\\\s*(revenue|profit|income|margin)\\\\b\",\n",
        "        r\"\\\\b(quarter|q[1-4]|fiscal|annual|yearly)\\\\b\",\n",
        "        r\"\\\\b(balance|financial|audit|compliance)\\\\b\",\n",
        "        r\"\\\\b(tax|liability|asset)\\\\b\",\n",
        "    ]\n",
        "    \n",
        "    OPERATIONAL_PATTERNS = [\n",
        "        r\"\\\\b(daily|weekly|monthly)\\\\b\",\n",
        "        r\"\\\\b(average|avg|mean)\\\\b\",\n",
        "        r\"\\\\b(count|number)\\\\s*(of)?\\\\s*(orders|customers|transactions)\\\\b\",\n",
        "        r\"\\\\b(top|best|worst)\\\\s*\\\\d*\",\n",
        "        r\"\\\\b(trend|pattern|comparison)\\\\b\",\n",
        "    ]\n",
        "    \n",
        "    def __init__(self):\n",
        "        self._critical = [re.compile(p, re.IGNORECASE) for p in self.CRITICAL_PATTERNS]\n",
        "        self._operational = [re.compile(p, re.IGNORECASE) for p in self.OPERATIONAL_PATTERNS]\n",
        "    \n",
        "    def classify(self, query, sql=None):\n",
        "        text = f\"{query} {sql or ''}\"\n",
        "        \n",
        "        critical_matches = sum(1 for p in self._critical if p.search(text))\n",
        "        operational_matches = sum(1 for p in self._operational if p.search(text))\n",
        "        \n",
        "        if critical_matches > 0:\n",
        "            return MaterialityTier.CRITICAL\n",
        "        elif operational_matches > 0:\n",
        "            return MaterialityTier.OPERATIONAL\n",
        "        else:\n",
        "            return MaterialityTier.INFORMATIONAL\n",
        "'''\n",
        "\n",
        "with open('/content/cwae_lib/classifier.py', 'w') as f:\n",
        "    f.write(classifier_code)\n",
        "\n",
        "# Write __init__.py\n",
        "init_code = '''\n",
        "from .types import MaterialityTier, MaterialityConfig, FMSResult\n",
        "from .fms import FMS\n",
        "from .classifier import MaterialityClassifier\n",
        "'''\n",
        "\n",
        "with open('/content/cwae_lib/__init__.py', 'w') as f:\n",
        "    f.write(init_code)\n",
        "\n",
        "sys.path.insert(0, '/content')\n",
        "print(\"âœ“ CWAE library created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the installation\n",
        "from cwae_lib import FMS, MaterialityClassifier, MaterialityTier\n",
        "\n",
        "classifier = MaterialityClassifier()\n",
        "tier = classifier.classify(\"What was Q3 total revenue?\")\n",
        "print(f\"âœ“ Classification works: '{tier.value}'\")\n",
        "\n",
        "fms = FMS(tier=tier)\n",
        "result = fms.score(10_234_567, 10_500_000)\n",
        "print(f\"âœ“ FMS scoring works: {result.score:.3f}\")\n",
        "print(f\"  (Binary accuracy would be: 0 - WRONG)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download BIRD Benchmark Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# BIRD dev set is available from their GitHub\n",
        "# https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird\n",
        "\n",
        "# Option 1: Download from BIRD's official source\n",
        "BIRD_DEV_URL = \"https://raw.githubusercontent.com/AlibabaResearch/DAMO-ConvAI/main/bird/data/dev/dev.json\"\n",
        "\n",
        "print(\"Downloading BIRD dev set...\")\n",
        "try:\n",
        "    response = requests.get(BIRD_DEV_URL, timeout=30)\n",
        "    if response.status_code == 200:\n",
        "        bird_data = response.json()\n",
        "        print(f\"âœ“ Downloaded {len(bird_data)} queries from BIRD dev set\")\n",
        "    else:\n",
        "        print(f\"Download failed with status {response.status_code}\")\n",
        "        bird_data = None\n",
        "except Exception as e:\n",
        "    print(f\"Download failed: {e}\")\n",
        "    bird_data = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: If download fails, use a representative sample\n",
        "# This is a subset of BIRD-style queries for demonstration\n",
        "\n",
        "if bird_data is None:\n",
        "    print(\"Using sample dataset for demonstration...\")\n",
        "    \n",
        "    # Sample queries representing different materiality tiers\n",
        "    bird_data = [\n",
        "        # Critical - Financial queries\n",
        "        {\"question\": \"What is the total revenue for Q3 2023?\", \"SQL\": \"SELECT SUM(amount) FROM transactions WHERE quarter = 'Q3'\", \"db_id\": \"financial\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Calculate the gross profit margin for last year\", \"SQL\": \"SELECT (revenue - cost) / revenue FROM annual_report\", \"db_id\": \"financial\", \"difficulty\": \"moderate\"},\n",
        "        {\"question\": \"What was the net income after tax?\", \"SQL\": \"SELECT net_income FROM financials WHERE year = 2023\", \"db_id\": \"financial\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Show total assets and liabilities\", \"SQL\": \"SELECT assets, liabilities FROM balance_sheet\", \"db_id\": \"financial\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What is the quarterly revenue breakdown?\", \"SQL\": \"SELECT quarter, SUM(revenue) FROM sales GROUP BY quarter\", \"db_id\": \"financial\", \"difficulty\": \"moderate\"},\n",
        "        {\"question\": \"Calculate EBITDA for the fiscal year\", \"SQL\": \"SELECT earnings + depreciation + amortization FROM financials\", \"db_id\": \"financial\", \"difficulty\": \"moderate\"},\n",
        "        {\"question\": \"What is our tax liability this quarter?\", \"SQL\": \"SELECT tax_amount FROM quarterly_taxes WHERE q = 'Q4'\", \"db_id\": \"financial\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Show the profit and loss statement\", \"SQL\": \"SELECT * FROM pnl_report WHERE year = 2023\", \"db_id\": \"financial\", \"difficulty\": \"simple\"},\n",
        "        \n",
        "        # Operational - Business metrics\n",
        "        {\"question\": \"How many orders were placed yesterday?\", \"SQL\": \"SELECT COUNT(*) FROM orders WHERE date = CURRENT_DATE - 1\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What is the average order value this month?\", \"SQL\": \"SELECT AVG(total) FROM orders WHERE month = 12\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Show top 10 customers by purchase amount\", \"SQL\": \"SELECT customer_id, SUM(amount) FROM orders GROUP BY customer_id ORDER BY 2 DESC LIMIT 10\", \"db_id\": \"retail\", \"difficulty\": \"moderate\"},\n",
        "        {\"question\": \"What is the daily sales trend this week?\", \"SQL\": \"SELECT date, SUM(amount) FROM sales WHERE date >= DATE_SUB(NOW(), 7) GROUP BY date\", \"db_id\": \"retail\", \"difficulty\": \"moderate\"},\n",
        "        {\"question\": \"Count of active customers in each region\", \"SQL\": \"SELECT region, COUNT(*) FROM customers WHERE status = 'active' GROUP BY region\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What is the monthly transaction count?\", \"SQL\": \"SELECT COUNT(*) FROM transactions WHERE MONTH(date) = 12\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Show the weekly sales comparison\", \"SQL\": \"SELECT week, SUM(sales) FROM weekly_report GROUP BY week\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What is the average customer lifetime value?\", \"SQL\": \"SELECT AVG(total_spent) FROM customer_ltv\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        \n",
        "        # Informational - Exploration queries\n",
        "        {\"question\": \"Show me the list of all products\", \"SQL\": \"SELECT * FROM products\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What categories do we have?\", \"SQL\": \"SELECT DISTINCT category FROM products\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"List all store locations\", \"SQL\": \"SELECT name, address FROM stores\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Show the database schema\", \"SQL\": \"DESCRIBE tables\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What columns are in the orders table?\", \"SQL\": \"DESCRIBE orders\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Preview the first 10 customer records\", \"SQL\": \"SELECT * FROM customers LIMIT 10\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"What are the product names?\", \"SQL\": \"SELECT name FROM products\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "        {\"question\": \"Show sample transaction data\", \"SQL\": \"SELECT * FROM transactions LIMIT 5\", \"db_id\": \"retail\", \"difficulty\": \"simple\"},\n",
        "    ]\n",
        "    \n",
        "    print(f\"âœ“ Created sample dataset with {len(bird_data)} queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the data\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(bird_data)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nSample queries:\")\n",
        "for i, row in df.head(5).iterrows():\n",
        "    print(f\"  {i+1}. {row['question'][:60]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Classify All Queries by Materiality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cwae_lib import MaterialityClassifier, MaterialityTier\n",
        "from collections import Counter\n",
        "\n",
        "classifier = MaterialityClassifier()\n",
        "\n",
        "# Classify each query\n",
        "classifications = []\n",
        "for item in bird_data:\n",
        "    question = item.get('question', '')\n",
        "    sql = item.get('SQL', item.get('query', ''))\n",
        "    tier = classifier.classify(question, sql)\n",
        "    classifications.append({\n",
        "        'question': question,\n",
        "        'sql': sql,\n",
        "        'tier': tier.value,\n",
        "        'tier_enum': tier\n",
        "    })\n",
        "\n",
        "# Count by tier\n",
        "tier_counts = Counter(c['tier'] for c in classifications)\n",
        "\n",
        "print(\"Materiality Distribution:\")\n",
        "print(f\"  Critical:      {tier_counts.get('critical', 0):4d} ({tier_counts.get('critical', 0)/len(classifications)*100:.1f}%)\")\n",
        "print(f\"  Operational:   {tier_counts.get('operational', 0):4d} ({tier_counts.get('operational', 0)/len(classifications)*100:.1f}%)\")\n",
        "print(f\"  Informational: {tier_counts.get('informational', 0):4d} ({tier_counts.get('informational', 0)/len(classifications)*100:.1f}%)\")\n",
        "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "print(f\"  Total:         {len(classifications):4d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show examples from each tier\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXAMPLES BY TIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for tier_name in ['critical', 'operational', 'informational']:\n",
        "    examples = [c for c in classifications if c['tier'] == tier_name][:3]\n",
        "    print(f\"\\n{tier_name.upper()}:\")\n",
        "    for ex in examples:\n",
        "        print(f\"  â€¢ {ex['question'][:70]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Simulate Model Predictions\n",
        "\n",
        "Since we don't have actual model outputs, we'll simulate predictions with realistic error patterns.\n",
        "\n",
        "**Simulation approach:**\n",
        "- Most queries are correct\n",
        "- Some have small numeric errors (within tolerance)\n",
        "- Some have larger errors (outside tolerance)\n",
        "- Error rates vary by model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def simulate_model_predictions(classifications, model_profile):\n",
        "    \"\"\"\n",
        "    Simulate model predictions with different error profiles.\n",
        "    \n",
        "    model_profile: dict with keys:\n",
        "        - accuracy: base accuracy rate\n",
        "        - critical_penalty: additional error rate on critical queries\n",
        "        - error_magnitude: typical error size when wrong\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    \n",
        "    for c in classifications:\n",
        "        tier = c['tier_enum']\n",
        "        \n",
        "        # Determine if this prediction is \"correct\" (exact match)\n",
        "        base_accuracy = model_profile['accuracy']\n",
        "        if tier == MaterialityTier.CRITICAL:\n",
        "            accuracy = base_accuracy - model_profile['critical_penalty']\n",
        "        else:\n",
        "            accuracy = base_accuracy\n",
        "        \n",
        "        is_exact_match = random.random() < accuracy\n",
        "        \n",
        "        # Simulate numeric values\n",
        "        gold_value = random.randint(1000, 1000000)  # Simulated gold value\n",
        "        \n",
        "        if is_exact_match:\n",
        "            predicted_value = gold_value\n",
        "            relative_error = 0.0\n",
        "        else:\n",
        "            # Generate error\n",
        "            error_pct = np.random.exponential(model_profile['error_magnitude'])\n",
        "            error_pct = min(error_pct, 0.5)  # Cap at 50%\n",
        "            direction = random.choice([-1, 1])\n",
        "            predicted_value = gold_value * (1 + direction * error_pct)\n",
        "            relative_error = error_pct\n",
        "        \n",
        "        predictions.append({\n",
        "            'question': c['question'],\n",
        "            'tier': tier,\n",
        "            'gold_value': gold_value,\n",
        "            'predicted_value': predicted_value,\n",
        "            'is_exact_match': is_exact_match,\n",
        "            'relative_error': relative_error\n",
        "        })\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Define different model profiles\n",
        "model_profiles = {\n",
        "    'Model_A': {\n",
        "        'accuracy': 0.92,         # 92% overall accuracy\n",
        "        'critical_penalty': 0.15, # But 15% worse on critical\n",
        "        'error_magnitude': 0.08   # 8% typical error\n",
        "    },\n",
        "    'Model_B': {\n",
        "        'accuracy': 0.88,         # 88% overall accuracy\n",
        "        'critical_penalty': -0.05, # Actually BETTER on critical (+5%)\n",
        "        'error_magnitude': 0.03   # Smaller errors when wrong\n",
        "    },\n",
        "    'Model_C': {\n",
        "        'accuracy': 0.90,         # 90% overall accuracy\n",
        "        'critical_penalty': 0.05, # Slightly worse on critical\n",
        "        'error_magnitude': 0.10   # Larger errors\n",
        "    }\n",
        "}\n",
        "\n",
        "# Generate predictions for each model\n",
        "all_predictions = {}\n",
        "for model_name, profile in model_profiles.items():\n",
        "    all_predictions[model_name] = simulate_model_predictions(classifications, profile)\n",
        "    print(f\"âœ“ Generated predictions for {model_name}\")\n",
        "\n",
        "print(f\"\\nTotal: {len(all_predictions)} models Ã— {len(classifications)} queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate with Both Standard EX and CWAE FMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cwae_lib import FMS, MaterialityTier\n",
        "\n",
        "def evaluate_model(predictions):\n",
        "    \"\"\"\n",
        "    Evaluate predictions with both EX and FMS.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'total': len(predictions),\n",
        "        'ex_correct': 0,\n",
        "        'fms_scores': [],\n",
        "        'by_tier': {\n",
        "            MaterialityTier.CRITICAL: {'count': 0, 'ex_correct': 0, 'fms_sum': 0},\n",
        "            MaterialityTier.OPERATIONAL: {'count': 0, 'ex_correct': 0, 'fms_sum': 0},\n",
        "            MaterialityTier.INFORMATIONAL: {'count': 0, 'ex_correct': 0, 'fms_sum': 0},\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    for pred in predictions:\n",
        "        tier = pred['tier']\n",
        "        \n",
        "        # Standard EX (binary)\n",
        "        ex = 1 if pred['is_exact_match'] else 0\n",
        "        results['ex_correct'] += ex\n",
        "        results['by_tier'][tier]['ex_correct'] += ex\n",
        "        results['by_tier'][tier]['count'] += 1\n",
        "        \n",
        "        # FMS (graduated)\n",
        "        fms = FMS(tier=tier)\n",
        "        if pred['gold_value'] != 0:\n",
        "            fms_result = fms.score(pred['predicted_value'], pred['gold_value'])\n",
        "        else:\n",
        "            fms_result = fms.score_binary(pred['is_exact_match'])\n",
        "        \n",
        "        results['fms_scores'].append(fms_result.score)\n",
        "        results['by_tier'][tier]['fms_sum'] += fms_result.score\n",
        "    \n",
        "    # Calculate aggregates\n",
        "    results['ex_accuracy'] = results['ex_correct'] / results['total']\n",
        "    results['avg_fms'] = sum(results['fms_scores']) / len(results['fms_scores'])\n",
        "    \n",
        "    # Calculate ETS (weighted by tier)\n",
        "    tier_weights = {\n",
        "        MaterialityTier.CRITICAL: 2.0,\n",
        "        MaterialityTier.OPERATIONAL: 1.0,\n",
        "        MaterialityTier.INFORMATIONAL: 0.5\n",
        "    }\n",
        "    \n",
        "    weighted_sum = 0\n",
        "    weight_total = 0\n",
        "    for tier, data in results['by_tier'].items():\n",
        "        if data['count'] > 0:\n",
        "            weighted_sum += tier_weights[tier] * data['fms_sum']\n",
        "            weight_total += tier_weights[tier] * data['count']\n",
        "    \n",
        "    results['ets'] = weighted_sum / weight_total if weight_total > 0 else 0\n",
        "    \n",
        "    # Tier-level metrics\n",
        "    for tier in MaterialityTier:\n",
        "        data = results['by_tier'][tier]\n",
        "        if data['count'] > 0:\n",
        "            data['ex_accuracy'] = data['ex_correct'] / data['count']\n",
        "            data['avg_fms'] = data['fms_sum'] / data['count']\n",
        "        else:\n",
        "            data['ex_accuracy'] = 0\n",
        "            data['avg_fms'] = 0\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Evaluate all models\n",
        "all_results = {}\n",
        "for model_name, predictions in all_predictions.items():\n",
        "    all_results[model_name] = evaluate_model(predictions)\n",
        "    print(f\"âœ“ Evaluated {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS COMPARISON: Standard EX vs CWAE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Header\n",
        "print(f\"\\n{'Model':<12} {'EX (Binary)':<15} {'ETS (CWAE)':<15} {'Critical FMS':<15} {'Î” Rank'}\")\n",
        "print(\"-\"*72)\n",
        "\n",
        "# Sort by EX\n",
        "ex_ranking = sorted(all_results.items(), key=lambda x: x[1]['ex_accuracy'], reverse=True)\n",
        "ex_ranks = {name: i+1 for i, (name, _) in enumerate(ex_ranking)}\n",
        "\n",
        "# Sort by ETS\n",
        "ets_ranking = sorted(all_results.items(), key=lambda x: x[1]['ets'], reverse=True)\n",
        "ets_ranks = {name: i+1 for i, (name, _) in enumerate(ets_ranking)}\n",
        "\n",
        "# Display\n",
        "for model_name, results in sorted(all_results.items()):\n",
        "    ex = results['ex_accuracy']\n",
        "    ets = results['ets']\n",
        "    critical_fms = results['by_tier'][MaterialityTier.CRITICAL]['avg_fms']\n",
        "    \n",
        "    rank_change = ex_ranks[model_name] - ets_ranks[model_name]\n",
        "    rank_str = f\"+{rank_change}\" if rank_change > 0 else str(rank_change)\n",
        "    \n",
        "    print(f\"{model_name:<12} {ex:>6.1%} (#{ex_ranks[model_name]})    {ets:>6.1%} (#{ets_ranks[model_name]})    {critical_fms:>6.1%}          {rank_str}\")\n",
        "\n",
        "print(\"-\"*72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed tier breakdown\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED BREAKDOWN BY MATERIALITY TIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for model_name, results in sorted(all_results.items()):\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  {'Tier':<15} {'Count':<8} {'EX Acc':<12} {'FMS Avg':<12}\")\n",
        "    print(f\"  {'-'*47}\")\n",
        "    \n",
        "    for tier in [MaterialityTier.CRITICAL, MaterialityTier.OPERATIONAL, MaterialityTier.INFORMATIONAL]:\n",
        "        data = results['by_tier'][tier]\n",
        "        print(f\"  {tier.value:<15} {data['count']:<8} {data['ex_accuracy']:>6.1%}       {data['avg_fms']:>6.1%}\")\n",
        "    \n",
        "    print(f\"  {'-'*47}\")\n",
        "    print(f\"  {'OVERALL':<15} {results['total']:<8} {results['ex_accuracy']:>6.1%}       {results['ets']:>6.1%} (ETS)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "models = list(all_results.keys())\n",
        "x = range(len(models))\n",
        "\n",
        "# Plot 1: EX vs ETS\n",
        "ax1 = axes[0]\n",
        "ex_scores = [all_results[m]['ex_accuracy'] for m in models]\n",
        "ets_scores = [all_results[m]['ets'] for m in models]\n",
        "\n",
        "width = 0.35\n",
        "ax1.bar([i - width/2 for i in x], ex_scores, width, label='EX (Binary)', color='steelblue')\n",
        "ax1.bar([i + width/2 for i in x], ets_scores, width, label='ETS (CWAE)', color='darkorange')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Overall: EX vs ETS')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(models)\n",
        "ax1.legend()\n",
        "ax1.set_ylim(0.7, 1.0)\n",
        "\n",
        "# Plot 2: Critical Query Performance\n",
        "ax2 = axes[1]\n",
        "critical_ex = [all_results[m]['by_tier'][MaterialityTier.CRITICAL]['ex_accuracy'] for m in models]\n",
        "critical_fms = [all_results[m]['by_tier'][MaterialityTier.CRITICAL]['avg_fms'] for m in models]\n",
        "\n",
        "ax2.bar([i - width/2 for i in x], critical_ex, width, label='EX', color='steelblue')\n",
        "ax2.bar([i + width/2 for i in x], critical_fms, width, label='FMS', color='darkorange')\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.set_title('Critical Queries Only')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(models)\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0.6, 1.0)\n",
        "\n",
        "# Plot 3: Ranking Comparison\n",
        "ax3 = axes[2]\n",
        "ex_rank_values = [ex_ranks[m] for m in models]\n",
        "ets_rank_values = [ets_ranks[m] for m in models]\n",
        "\n",
        "ax3.bar([i - width/2 for i in x], ex_rank_values, width, label='EX Rank', color='steelblue')\n",
        "ax3.bar([i + width/2 for i in x], ets_rank_values, width, label='ETS Rank', color='darkorange')\n",
        "ax3.set_ylabel('Rank (lower is better)')\n",
        "ax3.set_title('Ranking: EX vs ETS')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(models)\n",
        "ax3.legend()\n",
        "ax3.set_ylim(0, 4)\n",
        "ax3.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cwae_comparison.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Chart saved as 'cwae_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find ranking changes\n",
        "ranking_changes = []\n",
        "for model in models:\n",
        "    ex_rank = ex_ranks[model]\n",
        "    ets_rank = ets_ranks[model]\n",
        "    if ex_rank != ets_rank:\n",
        "        ranking_changes.append((model, ex_rank, ets_rank))\n",
        "\n",
        "if ranking_changes:\n",
        "    print(\"\\nðŸ“Š RANKING CHANGES:\")\n",
        "    for model, ex_r, ets_r in ranking_changes:\n",
        "        direction = \"â†‘\" if ets_r < ex_r else \"â†“\"\n",
        "        print(f\"   {model}: #{ex_r} (EX) â†’ #{ets_r} (ETS) {direction}\")\n",
        "\n",
        "# Find the model that looks best by EX but worst on critical\n",
        "ex_best = ex_ranking[0][0]\n",
        "critical_scores = [(m, all_results[m]['by_tier'][MaterialityTier.CRITICAL]['avg_fms']) for m in models]\n",
        "critical_ranking = sorted(critical_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\nâš ï¸  WARNING:\")\n",
        "print(f\"   {ex_best} ranks #1 by standard EX but may not be safest for production!\")\n",
        "print(f\"   On CRITICAL queries: {all_results[ex_best]['by_tier'][MaterialityTier.CRITICAL]['avg_fms']:.1%}\")\n",
        "print(f\"   Best on critical: {critical_ranking[0][0]} at {critical_ranking[0][1]:.1%}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ THE POINT:\")\n",
        "print(\"   Standard accuracy metrics can be misleading for enterprise AI.\")\n",
        "print(\"   A model optimized for overall accuracy may underperform on\")\n",
        "print(\"   the queries that matter most to your business.\")\n",
        "print(\"\\n   CWAE surfaces these risks by weighting errors by consequence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        'model': model,\n",
        "        'ex_accuracy': results['ex_accuracy'],\n",
        "        'ets': results['ets'],\n",
        "        'avg_fms': results['avg_fms'],\n",
        "        'critical_count': results['by_tier'][MaterialityTier.CRITICAL]['count'],\n",
        "        'critical_ex': results['by_tier'][MaterialityTier.CRITICAL]['ex_accuracy'],\n",
        "        'critical_fms': results['by_tier'][MaterialityTier.CRITICAL]['avg_fms'],\n",
        "        'operational_fms': results['by_tier'][MaterialityTier.OPERATIONAL]['avg_fms'],\n",
        "        'informational_fms': results['by_tier'][MaterialityTier.INFORMATIONAL]['avg_fms'],\n",
        "        'ex_rank': ex_ranks[model],\n",
        "        'ets_rank': ets_ranks[model],\n",
        "    }\n",
        "    for model, results in all_results.items()\n",
        "])\n",
        "\n",
        "results_df.to_csv('cwae_results.csv', index=False)\n",
        "print(\"âœ“ Results saved to 'cwae_results.csv'\")\n",
        "print(\"\\n\")\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "To validate with **real data**:\n",
        "\n",
        "1. **Get BIRD databases**: Download the actual SQLite files from BIRD\n",
        "2. **Get model predictions**: Use published model outputs or run models yourself\n",
        "3. **Execute SQL**: Actually run both gold and predicted SQL to get real values\n",
        "4. **Compare**: See if the ranking shift holds on real data\n",
        "\n",
        "The framework is ready - you just need real predictions to plug in!"
      ]
    }
  ]
}
